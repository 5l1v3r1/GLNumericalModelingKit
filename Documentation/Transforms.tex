\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{array}
\usepackage{varwidth}
\newcolumntype{L}{>{\varwidth[c]{\linewidth}}l<{\endvarwidth}}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{GLNumericalModelingKit}
\author{Jeffrey J. Early}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Introduction
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

GLNumericalModelingKit is an Objective-C framework designed to enable high performance numerical modeling of differential equations.

The primary design goal of this framework is to eliminate the tedious an error prone programming required for numerical modeling by introducing a layer of abstraction on top of the usual code. From the outset, the goal was to be able to simply write \texttt{[psi x]} in Objective-C and have that automatically take the x derivative of the variable psi, instead of fussing with memory buffers, differentiation matrices and Fourier transforms. GLNumericalModelingKit makes that possible, but also tries to be flexible enough to accommodate the unique demands that inevitably come with each project.

GLNumericalModelingKit consists of three major classes \emph{dimensions}, \emph{variables}, and \emph{variable operations} that map to familiar mathematical concepts. There are three types of variables, \emph{scalars} (which have no dimensions), \emph{functions} (which are defined on a set of dimensions) and \emph{linear transforms} (which transform functions between dimensions). The variable operations are used to perform all sorts of operations on the variables, including things such as addition, multiplication, interpolation, exponentiation, writing to files, Runge-Kutta time stepping, and so on. The linear transformations are used for Fourier, sine, cosine and Chebyshev transformations, differentiation, and other custom defined transformations. Taken together, these allow one to solve a fairly diverse set of mathematical problems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Mathematics}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The basic idea with numerical modeling to come up with a numerical approximation to some continuous function

\begin{table}
  \centering 
\begin{tabular}{|r|c|l|}
      \hline
      Hierarchy & Conceptual & Discipline \tabularnewline \hline \hline
      Level 0 & Continuous functions and differential equations & Calculus \tabularnewline \hline
      Level 1 & Abstract vectors and linear transformations & Linear algebra \tabularnewline \hline
      Level 2 & Vectors, matrices and matrix operations & Algorithm design \tabularnewline \hline
      Level 3 & Memory buffers and floating point operations & Programming \tabularnewline \hline
\end{tabular}
  \caption{Conceptual model for the different layers that go into numerical modeling. GLNumericalModelingKit handles layers 2 and 3, so the practitioner can focus on levels 0 and 1.}\label{conceptual_layers}
\end{table}

Numerical modeling requires that we represent continuous mathematical objects, such as functions and differential operators, as discrete objects, such as vectors and matrices. For the following sections dealing with the transformation of functions, we treat the discrete functions as vectors in a function space. The components of the functions are represented as superscripted contravariant tensors, $f^i$, where as the vector basis functions are represented as $\bar{e}_j$. Summation is assumed when an index appears in an expression twice, once as a superscript and once as a subscript.

In our notation all indices are zero based in order to be consistent with $C$ programming conventions and therefore run from $0$ through $n-1$. Superscripted indices indicate the row number, and subscripted indices indicate the column number.

Some examples include a column vector,
\begin{equation}
v^i = \left[\begin{array}{c}v_0 \\v_1 \\v_2 \\ \vdots \\v_{n-1}\end{array}\right]
\end{equation}
the Kronecker delta,
\begin{equation}
\delta^i_j = \left[\begin{array}{cccc}1 & 0 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & \ddots & 0 \\0 & 0 & 0 & 1\end{array}\right]
\end{equation}
the sub-diagonal Kronecker delta,
\begin{equation}
\delta^i_{j+1} = \delta^{i-1}_j = \left[\begin{array}{cccc}0 & 0 & 0 & 0 \\1 & 0 & 0 & 0 \\0 & \ddots & 0 & 0 \\0 & 0 & 1 & 0\end{array}\right]
\end{equation}
and the super-diagonal Kronecker delta,
\begin{equation}
\delta^i_{j-1} = \delta^{i+1}_j = \left[\begin{array}{cccc}0 & 1 & 0 & 0 \\0 & 0 & \ddots & 0 \\0 & 0 & 0 & 1 \\0 & 0 & 0 & 0\end{array}\right].
\end{equation}

A general sub-diagonal matrix can be represented as,
\begin{equation}
a^i \delta^i_{j+1} = a^{i} \delta^{i-1}_j = a_{j+1} \delta^i_{j+1} = a_{j+1} \delta^{i-1}_j = \left[\begin{array}{cccc}0 & 0 & 0 & 0 \\a_1 & 0 & 0 & 0 \\0 & \ddots & 0 & 0 \\0 & 0 & a_{n-1} & 0\end{array}\right]
\end{equation}
where the index $i$ is not summed because it does not appear as both a superscript and a subscript. A general super-diagonal matrix can similarly be represented as,
\begin{equation}
c^i \delta^i_{j-1} =c^{i+1} \delta^{i+1}_j = \left[\begin{array}{cccc}0 & c_0 & 0 & 0 \\0 & 0 & \ddots & 0 \\0 & 0 & 0 & c_{n-2} \\0 & 0 & 0 & 0\end{array}\right].
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Dimensions
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dimensions}

Dimensions are what you would expect, defining a coordinate with respect to some basis. Given that this is a numerical modeling framework, dimensions are defined on \emph{discrete} grids, rather than a continuous interval of some sort.

\subsection{Basis}

All dimensions are defined with respect some basis. This determines how a function is interpreted and how a function is differentiated. For example, the default basis is kGLDeltaBasis, conceptually a series of delta functions at each point.

At the heart of GLNumericalModelingKit is the ability to transform a variable between basis functions. If a one-dimensional function is defined on a dimension with basis kGLDeltaBasis, then you may transform this to kGLExponentialBasis or kGLCosineBasis, for example. The new coordinate defined with respect to the cosine basis, for example, should be thought of as coefficients of the cosine series. 

\begin{tabular}{llL}
      \hline
      Basis Name & Discrete value & Basis \tabularnewline \hline \hline
      \verb"kGLDeltaBasis" & $x_n$ &\[  \bar{x}_n \equiv  \delta( x - x_n) \] \tabularnewline
      \verb"kGLExponentialBasis" & $f_n$  & \[ \bar{e}_n \equiv e^{-2\pi i f_n x} \] \tabularnewline
      \verb"kGLSineBasis" & $f_n$ & \[ \bar{s}_n \equiv \sin( 2 \pi f_n x )\] \tabularnewline
      \verb"kGLCosineBasis" & $f_n$ & \[  \bar{c}_n \equiv \cos ( 2 \pi f_n x )\] \tabularnewline
      \verb"kGLChebyshevBasis"& $n$ & \[ \bar{t}_n \equiv  T_n(x) \] \tabularnewline 
             \verb"kGLDiscreteSineTransformIBasis" & $f_n$ & \[ \bar{s}_n \equiv \sin ( 2 \pi f_n x )\] \tabularnewline
      \verb"kGLDiscreteCosineTransformIBasis"&$f_n$ & \[ \bar{c}_n \equiv \cos ( 2 \pi f_n x ) \] \tabularnewline 
      \hline
\end{tabular}

The different sine and cosine basis are due to different assumptions about the symmetry of the underlying function.

\subsection{Grids}

A particular grid can be defined given the length $L$ and total number of points $N$. The following grids are supported,

\begin{tabular}{lllL}
      \hline
      Grid Name &$x_n-x_{\textrm{min}}$ & Use \tabularnewline \hline \hline
      \verb"kGLEndpointGrid" & $n \left(\frac{L}{N-1}\right)$ & sine \& cosine transform, finite difference \tabularnewline
      \verb"kGLInteriorGrid" & $\left(n + \frac{1}{2} \right) \left(\frac{L}{N}\right)$ & sine \& cosine half-shift transform \tabularnewline
      \verb"kGLPeriodicGrid" & $n \left(\frac{L}{N}\right)$ & exponential transform \tabularnewline
      \verb"kGLChebyshevEndpointGrid" & $ \frac{L}{2} \cos{ \left( \frac{\pi n}{N-1} \right) + \frac{L}{2}}$ & Chebyshev transform \tabularnewline
      \verb"kGLChebyshevInteriorGrid" & $\frac{L}{2} \cos{ \left( \frac{\pi (n+\frac{1}{2})}{N} \right) + \frac{L}{2} }$ & Chebyshev transform \tabularnewline
      \hline
\end{tabular}

There are a standard collection of grids that are used, some of which are well suited to transformations into a particular basis.

Note that the Chebyshev grids are monotonically \emph{decreasing}, a necessary choice given the fast cosine transformation implementations provided by FFTW.

Let's say we as for a grid with minimum of 0, a maximum 12 and a total of four points. This will result in the following values,

\begin{tabular}{llL}
      \hline
      Grid Name &Values \tabularnewline \hline \hline
      \verb"kGLEndpointGrid" & 0,4,8,12 \tabularnewline
      \verb"kGLInteriorGrid" & 1.5, 4.5, 7.5, 10.5 \tabularnewline
      \verb"kGLPeriodicGrid" & 0, 3, 6, 9 \tabularnewline
      \verb"kGLChebyshevEndpointGrid" & 12, 9, 3, 0 \tabularnewline
      \verb"kGLChebyshevInteriorGrid" & 11.54, 8.29, 3.70, 0.45 \tabularnewline
      \hline
\end{tabular}

\subsection{Periodicity}

Understanding periodicity in dimensions and how they related to the various transforms is fairly complicated.

It is important to understand how periodicity and domain length ($L$) affect the sample interval ($\Delta$) in a dimension. Mathematically,
\begin{equation}
\Delta =
\begin{cases}
\frac{L}{N}      	& \text{periodic}, \\
\frac{L}{N-1}       & \text{otherwise}.
\end{cases}
\end{equation}
where $N$ is the number of points. This reason for this distinction is because in a periodic domain the start point at $0$ is defined as being equal to the end point at $1$ and therefore we don't need to locate at both the start point and the end point. This is in contrast to an aperiodic dimension where the value of a function may differ at the two end points. In a periodic dimension you essentially get another sample point for free.

This distinction because very significant when you make a transformation to another basis, like an exponential, sine or cosine basis which assumes a particular form of periodicity.

An exponential basis always assumes that the domain is chopped into $N$ sample intervals and is thus directly compatible with the notion of a dimension being periodic. With the different types of sine and cosine basis, however, this isn't so simple. In particular, the DCT-I cosine transform assumes that the domain is chopped into $N-1$ sample intervals, while the DCT-II cosine transform assumes the domain is chopped into $N$ sample intervals. For this reason it's important to define a dimension correctly based on the transform you're intending to use.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Variables
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variables}

GLNumericalModelingKit defines three types of variables: scalars, functions and linear transformations. 

\subsection{Symmetry}

When a variable is created a symmetry can be specified which will reduce the number of points that need to be stored. For example, if a function is defined as having hermitian symmetry with respect to some dimension, then negative points do not need to be created as it is assumed that the function can be recovered at negative points by applying the symmetry. The same is true of both the even and odd symmetries.

\begin{tabular}{lL}
      \hline
      Symmetry Name & Definition \tabularnewline \hline \hline
      \verb"kGLNoSymmetry" &  $f(-x)=g(x)$ \tabularnewline
      \verb"kGLZeroSymmetry" &  \[ f(-x) = f(x) = 0 \] \tabularnewline
      \verb"kGLEvenSymmetry" &  \[ f(-x) = f(x) \] \tabularnewline
      \verb"kGLOddSymmetry" &  \[ f(-x) = - f(x)  \] \tabularnewline
      \hline
\end{tabular}

GLNumericalModelingKit is slightly more specific and actually defines a symmetry for the real part and the imaginary with respect to each dimension. Thus, a dimension with hermitian symmetry has even symmetry on the real part, and odd symmetry on the imaginary part.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Basis Transformations
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basis Transformations}
Forward transformations take a function $H^n$ defined in physical space on a grid $x_n$ and transform it to a function $h^k$ in spectral space on a grid $f_k$.

In general, a function is defined as,
\begin{equation}
f = h^n \bar{e}_n
\end{equation}
where $\bar{e}_n$ are the basis vectors and $h^n$ are the components. We can write the function with respect to a different basis, say $\bar{e}_{k^\prime}$, by inserting the transformation matrix and its inverse,
\begin{equation}
f = h^n \lambda^{k^\prime}_n \lambda^n_{k^\prime} \bar{e}_n
\end{equation}
which is just,
\begin{equation}
f = H^{n^\prime} \bar{e}_{n^\prime}
\end{equation}
if we define $H^{n^\prime} \equiv \lambda^{k^\prime}_n h^n$ and $\bar{e}_{n^\prime} \equiv  \lambda^n_{k^\prime} \bar{e}_n$.

\begin{tabular}{llL}
      \hline
      Basis & FFTW & Forward Transformation \tabularnewline \hline \hline
      \verb"kGLExponentialBasis" & \verb"FFTW_FORWARD" & \[ h_k = \sum_{n=0}^{N-1} H_n e^{-2\pi i k n/N} \] \tabularnewline
      \verb"kGLSineBasis" & \verb"RODFT10" & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \sin \left[ \pi(n+1/2)(k+1)/N\right] \] \tabularnewline
      \verb"kGLCosineBasis" & \verb"REDFT10" & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \cos \left[ \pi (n+1/2)k/N\right] \] \tabularnewline
      \verb"kGLDiscreteSineTransformIBasis" & \verb"RODFT00" & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \sin \left[ \pi(n+1)(k+1)/(N+1)\right] \] \tabularnewline
      \verb"kGLDiscreteCosineTransformIBasis" & \verb"REDFT00" & \[ h_k = H_{0} + (-1)^{k} H_{n-1} +  2 \sum_{n=1}^{N-2} H_n \cos \left[ \pi n k/(N-1)\right] \] \tabularnewline
      \hline
\end{tabular}

Given function $h_k$ in the frequency domain, the following transforms return function $H_n$ in the spatial domain.

\begin{tabular}{llL}
      \hline
      Basis & FFTW & Inverse Transformation \tabularnewline \hline \hline
      \verb"kGLExponentialBasis" & \verb"FFTW_INVERSE" & \[ H_n = \sum_{k=0}^{N-1} h_k e^{2\pi i k n/N} \] \tabularnewline
      \verb"kGLSineBasis" & \verb"RODFT01" & \[ H_n = (-1)^n h_{N-1} + 2 \sum_{k=0}^{N-2} h_k \sin \left[ \pi(k+1)(n+1/2)/N\right] \] \tabularnewline
      \verb"kGLCosineBasis" & \verb"REDFT01" & \[ H_n = h_{0} + 2 \sum_{k=1}^{N-1} h_k \cos \left[ \pi (n+1/2)k/N\right] \] \tabularnewline
      \verb"kGLDiscreteSineTransformIBasis" & \verb"RODFT00" & \[ H_n = 2 \sum_{k=0}^{N-1} h_k \sin \left[ \pi(n+1)(k+1)/(N+1)\right] \] \tabularnewline
      \verb"kGLDiscreteCosineTransformIBasis" & \verb"REDFT00" & \[ H_n = h_{0} + (-1)^{n} h_{k-1} +  2 \sum_{k=1}^{N-2} h_k \cos \left[ \pi n k/(N-1)\right] \] \tabularnewline
      \hline
\end{tabular}

Each of these can be written in terms of a frequency and position, although we will ignore the two transformations that are not in the half-shift basis.

\begin{tabular}{lllL}
      \hline
      Basis &$x_n$ & $f_k$ & Forward Transformation \tabularnewline \hline \hline
      \verb"kGLExponentialBasis" & $n\Delta$ & $\frac{k}{N\Delta}$ & \[ h_k = \sum_{n=0}^{N-1} H_n e^{-2\pi i f_k x_n} \] \tabularnewline
      \verb"kGLSineBasis" & $ n\Delta + \frac{\Delta}{2}$ & $\frac{k+1}{2N\Delta}$ & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \sin \left[ 2 \pi f_k x_n \right] \] \tabularnewline
      \verb"kGLCosineBasis" & $ n\Delta + \frac{\Delta}{2}$ & $\frac{k}{2N\Delta}$ & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \cos \left[ 2 \pi f_k x_n \right] \] \tabularnewline
       \verb"kGLDiscreteSineTransformIBasis" & $n\Delta + \Delta$ & $\frac{k+1}{2 \Delta (N+1)}$ & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \sin \left[ 2 \pi f_k x_n \right] \] \tabularnewline
      \verb"kGLDiscreteCosineTransformIBasis"&$n\Delta$ & $\frac{k}{2 \Delta (N-1)}$ & \[ h_k = H_{0} + (-1)^{k} H_{n-1} +  2 \sum_{n=1}^{N-2} H_n \cos \left[ 2 \pi f_k x_n \right] \] \tabularnewline 
      \hline
\end{tabular}

\begin{tabular}{lllL}
      \hline
      Basis &$x_n$ & $f_k$ & Inverse Transformation \tabularnewline \hline \hline
      \verb"kGLExponentialBasis" & $n\Delta$ & $\frac{k}{N\Delta}$ & \[ H_n = \sum_{k=0}^{N-1} h_k e^{2\pi i f_k x_n} \] \tabularnewline
      \verb"kGLSineBasis" & $ n\Delta + \frac{\Delta}{2}$ & $\frac{k+1}{2N\Delta}$ & \[ H_n = (-1)^n h_{N-1} + 2 \sum_{k=0}^{N-2} h_k \sin \left[ 2 \pi f_k x_n \right] \] \tabularnewline
      \verb"kGLCosineBasis" & $ n\Delta + \frac{\Delta}{2}$ & $\frac{k}{2N\Delta}$ & \[ H_n = h_{0} + 2 \sum_{k=1}^{N-1} h_k \cos \left[ 2 \pi f_k x_n \right] \] \tabularnewline
      \verb"kGLDiscreteSineTransformIBasis" & $n\Delta + \Delta$ & $\frac{k+1}{2 \Delta (N+1)}$ & \[ H_n = 2 \sum_{k=0}^{N-1} h_k \sin \left[ 2 \pi f_k x_n \right] \] \tabularnewline
      \verb"kGLDiscreteCosineTransformIBasis"&$n\Delta$ & $\frac{k}{2 \Delta (N-1)}$ & \[ H_n = h_{0} + (-1)^{n} h_{n-1} +  2 \sum_{k=1}^{N-2} h_k \cos \left[ 2 \pi f_k x_n \right] \] \tabularnewline 
      \hline
\end{tabular}

One thing to notice about the sine and cosine transforms in the \emph{half-shift basis} is that they have the same collocation points and that they're defined at the same frequencies. The only difference is that the sine basis doesn't have a zero frequency, and the cosine basis doesn't have the Nyquist frequency. One can translate between the two frequencies using that that $f_k^{\textrm{sine}}=f_{k+1}^{\textrm{cosine}}$, or simply $f_k^{\textrm{s}}=f_{k+1}^{\textrm{c}}$ for short.

In contrast, the sine and cosine basis transforms are not defined at the same frequencies

\begin{tabular}{lL}
      \hline
      Basis & Forward Transformation \tabularnewline \hline \hline
      \verb"kGLDiscreteSineTransformIBasis" & \[ h_k = 2 \sum_{n=0}^{N-1} H_n \sin \left[ \pi(n+1)(k+1)/(N+1)\right] \] \tabularnewline
      \verb"kGLDiscreteCosineTransformIBasis" & \[ h_k = H_{0} + (-1)^{k} H_{n-1} +  2 \sum_{n=1}^{N-2} H_n \cos \left[ \pi n k/(N-1)\right] \] \tabularnewline
      \hline
\end{tabular}

\subsection{Chebyshev transformation}
The Chebyshev polynomials are defined as,
\begin{equation}
T_k(x) = \cos{ \left( k \cos^{-1}{x} \right)}.
\end{equation}
The normalization coefficients $c_k$ are are found from,
\begin{equation}
\int_{-1}^{1} T_k^2(x) \frac{dx}{\sqrt{1-x^2}} = c_k \frac{\pi}{2}
\end{equation}
and are,
\begin{equation}
c_k = \begin{cases}
    2  & k=0, \\
    1  & \text{otherwise}.
\end{cases}
\end{equation}
With this, we can write a function $u(x)$ in a Chebyshev basis as
\begin{equation}
u(x) = \sum_{k=0} \hat{u}_k T_k(x)
\end{equation}
using that,
\begin{equation}
\hat{u}_k = \frac{2}{\pi c_k} \int_{-1}^{1} u(x) T_k(x) \frac{dx}{\sqrt{1-x^2}}.
\end{equation}

According to Canuto equation 2.4.15, for the Gauss-Lobatto grid the discrete Chebyshev transformation is,
\begin{equation}
C_{kn} = \frac{2}{(N-1) c_n c_k} \cos \frac{\pi n k}{N-1}
\end{equation}
where now the normalization coefficients are,
\begin{equation}
c_n = \begin{cases}
    2  & n=0, N-1 \\
    1  & \text{otherwise}.
\end{cases}
\end{equation}

Written out a bit more, this means that we can find the spectral component $\hat{u}_k$ with
\begin{equation}
\hat{u}_k = \frac{1}{(N-1) c_k} \left[ u_0 + (-1)^{k} u_{N-1} +  2 \sum_{n=1}^{N-2} u_n \cos\frac{\pi n k}{N-1} \right],
\end{equation}
which is nearly the fast cosine transform implemented in FFTW above. We are actually going to compute,
\begin{equation}
\hat{h}_k = \frac{1}{2(N-1)} \left[ u_0 + (-1)^{k} u_{N-1} +  2 \sum_{n=1}^{N-2} u_n \cos\frac{\pi n k}{N-1} \right],
\end{equation}
which means that $\hat{h}_k = \frac{c_k \hat{u}_k}{2}$. We want the inverse transform to be
\begin{equation}
u_n = \sum_{k=0}^{N-1} \hat{u}_k \cos \frac{\pi n k}{N-1}.
\end{equation}
Written out,
\begin{equation}
u_n = \hat{u}_0 + (-1)^{n}   \hat{u}_{N-1} + \sum_{k=1}^{N-2} \hat{u}_k \cos \frac{\pi n k}{N-1}.
\end{equation}
or, if we use $\hat{h}_k$ instead,
\begin{equation}
u_n = \hat{h}_0 + (-1)^{n} \hat{h}_{N-1} + 2 \sum_{k=1}^{N-2} \hat{h}_k \cos \frac{\pi n k}{N-1}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Fast Transforms
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fast Transforms}

In addition to the discrete linear transformation (which typically requires $n^2$ operations), some transformations may have a \emph{fast transform} algorithm defined. The fast algorithm is provides the same result as doing the slower transformation, but requires fewer operations. In particular, the Fourier, sine, cosine, and Chebyshev transformation all have corresponding fast transforms which require only $n \log n$ operations. Finite differencing operation can also be defined as a banded diagonal matrix, or as some memory local stencil which may be significantly faster.

GLNumericalModelingKit does not implement the fast algorithms directly, but instead uses external libraries such as FFTW.

\subsection{FFTW transformations}

\subsubsection{Forward Transforms}

\begin{itemize}
\item \textbf{Real-to-real} transforms can \emph{only} act on variables that are purely real and \emph{only} on dimensions in the kGLDeltaBasis. These transformations are the sine and cosine basis transformations.
\item \textbf{Real-to-complex} transforms can \emph{only} act on variables that are purely real and \emph{only} on dimensions in the kGLDeltaBasis. The transformed dimensions will be in the kGLExponentialBasis. The last transformed dimension will have strictly positive frequencies, exploiting the underlying Hermitian symmetry where the real part is even symmetric and the imaginary part odd symmetric.
\item \textbf{Complex-to-complex} transforms can \emph{only} act on complex variables, although they may be purely real or otherwise. A complex-to-complex transformation will only transform dimensions in the kGLDeltaBasis to the kGLExponentialBasis. Furthermore, this transform will not act on a complex variable with existing Hermitian symmetry on other dimensions in an exponential basis to prevent confusion.

\end{itemize}

\subsubsection{Inverse Transforms}

\begin{itemize}
\item \textbf{Real-to-real} transforms can \emph{only} act on variables that are purely real and \emph{only} on dimensions in the sine and cosine basis, returning them to the kGLDeltaBasis.
\item \textbf{Complex-to-real} transforms can \emph{only} act on variables with a Hermitian symmetry defined on dimensions in the kGLExponentialBasis. The transformed dimensions will be in the kGLDeltaBasis. The last transformed dimension must have strictly positive frequencies.
\item \textbf{Complex-to-complex} transforms can \emph{only} act on complex variables, although they may be purely real or otherwise. A complex-to-complex transformation will only transform dimensions in the kGLExponentialBasis to the kGLDeltaBasis. Furthermore, this transform will not act on a complex variable with existing Hermitian symmetry on other dimensions in an exponential basis? Why not?

\end{itemize}

\subsection{Wisdom}

FFTW has acquires `wisdom' about the optimal execution algorithm that should be saved in certain cases. We can and should do this on a per-machine basis in the Application Support folder.



\subsection{Data Format}

A separate concept to this is the \emph{data format}. In GLNumericalModelingKit we current have three different data formats: purely real, split complex, and half complex. It's temping to support interleaved because this corresponds directly to the C99 imaginary number definition which may make for easy programming. The half complex format is defined to exist on a particular dimension. FFTW automatically puts it on the last dimension in the real-to-complex operations, but it can be on any (or multiple!) dimensions in the real-to-real transforms. We will *not* support half-complex format on multiple dimensions.

So now we need to consider the different  transformations that are possible, and how we will actually perform these in practice.
\begin{itemize}
\item \textbf{real-to-real} In this case we'll be taking a real function and transforming it to a mixed sine/cosine basis. This is easy and we just use \verb"fftw_plan_guru_r2r".
\item \textbf{real-to-exponential} In this case we'll be taking a real function and transforming it to an exponential basis. I believe we can just use \verb"fftw_plan_guru_split_dft_r2c". We need to check if the complex output really is reduced in size. It appears so.
\item \textbf{real-to-mixed} There are actually two sub cases here. In one case, exactly \emph{one} dimension will transform to an exponential basis. In this case we can can use \verb"fftw_plan_guru_r2r" and end up with half-complex data formatting on that dimension. In the second case, two or more dimensions need to be transformed to an exponential basis. Here, we should be able to transform first with \verb"fftw_plan_guru_r2r" set to stride out the untransformed dimensions. This is then followed by \verb"fftw_plan_guru_split_dft_r2c" set to stride out the real dimensions.
\end{itemize}

If I'm thinking of this correctly, we can always do a real-to-real transform first on the appropriate dimensions, and then 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
\section{Differential Transformations}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A differential operators acts on a variable and returns a new differentiated variable. The method of differentiation that is used can be set by choosing a basis for differentiation. For example, choosing kGLDeltaBasis will use finite differencing, whereas kGLExponentialBasis will do a Fourier transform and differentiate in wavenumber space.

A default differentiation basis can be selected for a given equation, but can be overridden on a variable by variable case. Internally, GLNumericalModelingKit uses the following logic to determine which differential operator to use.

\begin{enumerate}
\item User requests differentiation with a string such as `xx' or `laplacian'.
\item GLDifferentialOperatorPool looks for a cached version of the operator in the pool associated with the preferred differentiation basis.
\item If a cached version can't be found, GLDifferentialOperatorPool checks to see if it knows how to build the requested operator.
\item Otherwise it throws an exception.
\end{enumerate}

An individual differential operator knows how to differentiate a specific set of dimensions---typically spatial dimensions such as $x$, $y$ and $z$---which we denote as the \emph{differentiable dimensions}. However, the operator may need to transform the variable to a different basis in order to actually carry out the differentiation, these are the \emph{transformed dimensions}. Thus, so long as the variable can be transformed to the \emph{transformed dimensions}, then the operator is capable of differentiating that variable.

More specifically then, when the GLDifferentialOperatorPool looks for a cached version of an operator for a particular variable it must
\begin{enumerate}
\item return a list of operators with that name associated with the preferred differentiation basis,
\item determine whether or not the variables dimensions can be transformed into the operators \emph{transformed dimensions}.
\end{enumerate}

This particular search algorithm allows the user to override internal instructions for building an operator, by simply caching the operator with that name.

A GLDifferentialOperatorPool is specific to a particular set of \emph{differentiable dimensions} and \emph{transformed dimensions}

\textbf{Important note:} The advantage to using the half-shift basis, aside from speed, is that the colocation points in the spatial domain are the same. This is \emph{not} the case for the basic sine and cosine transform. The basic cosine transform requires the function be specified at the end points of the domain, whereas the sine transform does not. This effectively gives the sine transform two more points to work with and also therefore makes it difficult to define an obvious transformation between the two different basis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Implementation}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Designing the GLDifferentialOperator class is actually quite tricky. On the one hand, a differential operator is obviously a unary operator---it takes a single variable and returns a single differentiated variable. On the other hand, differential operators are variables---wavenumber vectors for spectral differentiation or sparse differentiation matrices for finite differencing.

To get the best of both worlds, the GLDifferentialOperator class is a subclass of GLVariableOperation that has a GLVariable property to store the differentiation matrix.

The role of the GLDifferentialOperatorPool is slightly expanded. The pool is now able to store and return differential operators, as well as differential operation matrices. When a differential matrix is set, the \emph{toBasis} must also be set (the \emph{fromBasis} is already defined at the operator pool level). This requires the mathematician be much more aware of what they are doing, which I think is good. When a differential operator is requested (of the same name as the matrix), then a new operation is created with the right matrix and toBasis.

The best shortcut for differentiation would look like this,

-[GLVariable differentiateWithOperator: (NSString *) opName byTransformingToBasis: (NSArray *) orderedBasis];

When do I initialize the operator? It's going to be a slightly different prototype. We init

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Differentiation Matrices}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A function in delta basis may be represented as,
\begin{equation}
f = H^n \bar{x}_n
\end{equation}
or as
\begin{equation}
f = h^k \bar{e}_k
\end{equation}
in some spectral basis such as the exponentials in the Fourier series. The goal in this section is determine how we need to transform the components in function in the spectral basis in order to take the derivative $\partial_x$. So, we seek to determine the differentiation matrix $D_x$ by computing
\begin{equation}
\partial_x f= \partial_x\left( h^k \bar{e}_k \right).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{kGLExponentialBasis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Starting with,
\begin{equation}
f = h^k \bar{e}_k
\end{equation}
we compute its derivative with respect to $x$,
\begin{align}
\partial_x f &= \partial_x  \left( \sum_{k=0}^{N-1} h^k e^{2\pi i f_k^{\textrm{e}} x} \right)\\
&=  \sum_{k=0}^{N-1} h^k (2 \pi i f_k^{\textrm{e}} ) e^{2\pi i f_k x} \\
& =  \sum_{k=0}^{N-1} \left[D_x^{\textrm{e}} \right]^k_m h^m e^{2\pi i f_k x}.
\end{align}
This means that the differentiation operator $D_x^{\textrm{e}}$ is defined as,
\begin{equation}
\left[D_x^{\textrm{e}} \right]^k_m \equiv \left( 2 \pi i f_m^{\textrm{e}} \right) \delta_{m}^k .
\end{equation}
and takes the components of a function in the exponential basis and transforms it to the components of a differentiated function in the exponential basis.

This linear transformation can be represented by a diagonal matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{kGLSineBasis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Starting with,
\begin{equation}
f = h^k \bar{s}_k
\end{equation}
and taking the derivative with respect to $x$,
\begin{align}
\partial_x f &= \partial_x  \left( (-1)^n h^{N-1} + 2 \sum_{k=0}^{N-2} h^k \sin \left[ 2 \pi f_k^{\textrm{s}} x \right] \right)\\
&= 2 \sum_{k=0}^{N-2} h^k \left( 2 \pi f_k^{\textrm{s}} \right) \cos \left[ 2 \pi f_k^{\textrm{s}} x \right]
\end{align}
Now let $k=m-1$, so that
\begin{align}
\partial_x f &= 2 \sum_{m=1}^{N-1}  h^k \left( 2 \pi f_k^{\textrm{s}} \right)\cos \left[ 2 \pi f_{m-1}^{\textrm{s}} x \right] \\
&= 2 \sum_{m=1}^{N-1}  h^k \left( 2 \pi f_k^{\textrm{s}} \right) \cos \left[ 2 \pi f_m^{\textrm{c}} x \right]
\end{align}
where we used that $f_k^{\textrm{s}}=f_{k+1}^{\textrm{c}}$ in the last step. Let's define $g^m$ to be the coefficient of the cosine series. In that case,
\begin{equation}
g^m = \left( 2 \pi f_k^{\textrm{s}} \right) \delta^{m-1}_k h^k .
\end{equation}
This means that the differentiation operator $D_x^{\textrm{s}}$ is defined as,
\begin{equation}
\left[D_x^{\textrm{s}}\right]^m_k \equiv \left( 2 \pi f_k^{\textrm{s}} \right) \delta^{m-1}_k.
\end{equation}
This operator takes a the components of a function defined in a sine basis, and transforms it to the components of a differentiated function in the cosine basis.

This linear transformation can be represented by a subdiagonal matrix. To be explicit,
\begin{equation}
\left( 2 \pi f_k^{\textrm{s}} \right) \delta^{m-1}_k = 2 \pi \left[\begin{array}{ccccc}0 & 0 & 0 & 0 & 0 \\f_0^{\textrm{s}} & 0 & 0 & 0 & 0 \\0 & f_1^{\textrm{s}} & 0 & 0 & 0 \\0 & 0 & \ddots & 0 & 0 \\0 & 0 & 0 & f_{N-2}^{\textrm{s}} & 0\end{array}\right].
\end{equation}

Using our storage format for sub-diagonal matrices, this is just $f^c_k$. The $k=0$ component is ignored.

Higher derivatives with respect to $x$ result in, 
\begin{align}
\left[D_{xx}^{\textrm{s}}\right]^m_k \equiv& - \left( 2 \pi f_k^{\textrm{s}} \right)^2 \delta^{m}_k \\
\left[D_{xxx}^{\textrm{s}}\right]^m_k \equiv&  - \left( 2 \pi f_k^{\textrm{s}} \right)^3 \delta^{m+1}_k. \\
\left[D_{xxxx}^{\textrm{s}}\right]^m_k \equiv& \left( 2 \pi f_k^{\textrm{s}} \right)^4 \delta^{m}_k 
\end{align}
and so on.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{kGLCosineBasis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We denote the $k$ frequency component in the cosine basis by $h_k^{\textrm{c}}$.
\begin{align}
\partial_x f&= \partial_x  \left( h_{0} + 2 \sum_{k=1}^{N-1}h^k \cos \left[ 2 \pi f_k^{\textrm{c}} x \right] \right)\\
&= 2 \sum_{k=1}^{N-1} h^k \left( -2 \pi f_k^{\textrm{c}} \right) \sin \left[ 2 \pi f_k^{\textrm{c}} x \right]
\end{align}
Now let $k=m+1$, so that
\begin{align}
\partial_x f &= 2 \sum_{m=0}^{N-2}  h^k \left( -2 \pi f_k^{\textrm{c}} \right)\sin \left[ 2 \pi f_{m+1}^{\textrm{c}} x_n \right] \\
&= 2 \sum_{m=0}^{N-2}  h^k \left( -2 \pi f_k^{\textrm{c}} \right) \sin \left[ 2 \pi f_m^{\textrm{s}} x_n \right]
\end{align}
where we used that $f_k^{\textrm{s}}=f_{k+1}^{\textrm{c}}$ in the last step. Let's define $g^m$ to be the coefficient of the sum. In that case,
\begin{equation}
g^m = \delta^{m+1}_k h^k \left( -2 \pi f_k^{\textrm{c}} \right).
\end{equation}
This means that the differentiation operator $D_x^{\textrm{c}}$ is defined as,
\begin{equation}
\left[D_x^{\textrm{c}}\right]^m_k \equiv - \left( 2 \pi f_k^{\textrm{c}} \right) \delta^{m+1}_k.
\end{equation}
This operator takes a function defined in a cosine basis, and transforms it to a differentiated function in the sine basis.

This linear transformation can be represented by a superdiagonal matrix.To be explicit,
\begin{equation}
\left( -2 \pi f_k^{\textrm{c}} \right) \delta^{m+1}_k = -2 \pi \left[\begin{array}{ccccc}0 & f_1^{\textrm{c}} & 0 & 0 & 0 \\0 & 0 & f_2^{\textrm{c}} & 0 & 0 \\0 & 0 & 0 & \ddots & 0 \\0 & 0 & 0 & 0 & f_{N-1}^{\textrm{c}} \\0 & 0 & 0 & 0 & 0\end{array}\right].
\end{equation}

Using our storage format for sub-diagonal matrices, this is just $f^s_k$. The $k=N-1$ component is ignored.

Higher derivatives with respect to $x$ result in, 
\begin{align}
\left[D_{xx}^{\textrm{c}}\right]^m_k \equiv& - \left( 2 \pi f_k^{\textrm{c}} \right)^2 \delta^{m}_k \\
\left[D_{xxx}^{\textrm{c}}\right]^m_k \equiv&  \left( 2 \pi f_k^{\textrm{c}} \right)^3 \delta^{m+1}_k. \\
\left[D_{xxxx}^{\textrm{c}}\right]^m_k \equiv& \left( 2 \pi f_k^{\textrm{c}} \right)^4 \delta^{m}_k 
\end{align}
and so on.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{kGLChebyshevBasis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We have the series,
\begin{equation}
u_n=\sum_{k=0}^{N-1} \hat{u}_k T_k(x_n)
\end{equation}
and we want to find its derivative. We can show that the following relationship is true,
\begin{equation}
2 T_k = \frac{c_k}{k+1} T_{k+1}^\prime - \frac{1}{k-1} T_{k-1}^\prime
\end{equation}
where the $T_k^\prime$ polynomials are zero for $k<1$ and $k>N-1$. If we denote the coefficients for the derivative of the function as $\hat{u}_k^{(1)}$, then by definition
\begin{align}
 \sum_{k=0}^{N-1} \hat{u}_kT_k^\prime(x_n) =& \sum_{k=0}^{N-1} \hat{u}_k^{(1)} T_k(x_n) \\
 =& \frac{1}{2} \sum_{k=0}^{N-1} \hat{u}_k^{(1)} \left(\frac{c_k}{k+1} T_{k+1}^\prime(x_n) - \frac{1}{k-1} T_{k-1}^\prime(x_n) \right) \\
 =& \frac{1}{2} \sum_{k=0}^{N} \hat{u}_{k-1}^{(1)} \frac{c_{k-1}}{k} T_{k}^\prime(x_n) -  \sum_{k=-1}^{N-2} \hat{u}_{k+1}^{(1)} \frac{1}{k} T_{k}^\prime(x_n) \\
 =& \frac{1}{2} \sum_{k=0}^{N-1} \frac{1}{k} \left( c_{k-1} \hat{u}_{k-1}^{(1)} -  \hat{u}_{k+1}^{(1)} \right) T_{k}^\prime(x_n) 
\end{align}
and we conclude that,
\begin{equation}
2 k \hat{u}_k = c_{k-1} \hat{u}_{k-1}^{(1)} -  \hat{u}_{k+1}^{(1)}.
\end{equation}
The appropriate operator for computing the derivative is thus most efficiently defined as the recursion,
\begin{equation}
c_{k} \hat{u}_{k}^{(1)} = 2 (k+1) \hat{u}_{k+1} + \hat{u}_{k+2}^{(1)}
\end{equation}
To properly scale this, a factor of $\frac{L}{2}$ needs to be out front each derivative term. So that,
\begin{equation}
c_{k} \hat{u}_{k}^{(1)} = \frac{4}{L} (k+1) \hat{u}_{k+1} + \hat{u}_{k+2}^{(1)}.
\end{equation}
But we can do even better, and write this in terms of what we're actually computing, $\hat{h}_k = \frac{c_k \hat{u}_k}{2}$, so that,
\begin{equation}
\hat{h}_{k}^{(1)} = \frac{4}{L} (k+1) \hat{h}_{k+1} + \hat{h}_{k+2}^{(1)}.
\end{equation}
Note that we can drop the $c_k$ factor on the terms on the right-hand-side because they're all greater than $0$ where $c_k=1$. Let's write out a few of these to get a sense of what they look like.
\begin{align}
\hat{h}_{N-1}^{(1)} =& 0 \\
\hat{h}_{N-2}^{(1)} =& \frac{4}{L} \left(  (N-1) \hat{h}_{N-1} \right) \\
\hat{h}_{N-3}^{(1)} =& \frac{4}{L} \left(  (N-2) \hat{h}_{N-2} \right) \\
\hat{h}_{N-4}^{(1)} =& \frac{4}{L} \left(  (N-3) \hat{h}_{N-3} +  (N-1) \hat{h}_{N-1} \right) \\
\hat{h}_{N-5}^{(1)} =& \frac{4}{L} \left(  (N-4) \hat{h}_{N-4} + (N-2) \hat{h}_{N-2} \right) 
\end{align}

We can write this as a transformation matrix,
\begin{equation}
\hat{h}_{k}^{(1)} = \frac{4}{L} \sum_{p=k+1, p+k odd} p \hat{h}_p
\end{equation}

The obvious thing to do is to set the frequencies to $f_k = \frac{2k}{L}$. In this case,
\begin{equation}
\hat{h}_{k}^{(1)} = \sum_{p=k+1, p+k odd} 2 f_p \hat{h}_p
\end{equation}

\subsubsection{Other}
Taking derivatives of the function in the different basis,
\begin{equation}
h_k^\prime = \sum_{n=0}^{N-1} H_n \left( \frac{2 \pi i n}{N} \right) e^{2\pi i k n/N}
\end{equation}
a kGLDiscreteSineTransformIBasis,
\begin{align}
h_k^\prime  &= 2 \sum_{n=0}^{N-1} H_n \left( \frac{\pi (n+1)}{N+1} \right) \cos \left[ \pi(n+1)(k+1)/(N+1)\right] \\
&= 2 \sum_{n=1}^{N-1} H_{n-1} \left( \frac{\pi n}{N} \right) \cos \left[ \pi n (k+1/2)/N\right]
\end{align}
a kGLSineBasis,
\begin{align}
h_k &= (-1)^k H_{N-1} + 2 \sum_{n=0}^{N-2} H_n \sin \left[ \pi(n+1)(k+1/2)/N\right] \\
h_k^\prime &=2 \sum_{n=0}^{N-2} H_n  \left( \frac{\pi (n+1)}{N} \right) \cos \left[ \pi(n+1)(k+1/2)/N\right] \\
&=2 \sum_{n=1}^{N-1} H_{n-1}  \left( \frac{\pi n}{N} \right) \cos \left[ \pi n(k+1/2)/N\right]
\end{align}
and a cosine basis,
\begin{align}
h_k &= H_{0} + 2 \sum_{n=1}^{N-1} H_n \cos \left[ \pi n(k+1/2)/N\right] \\
h_k^\prime &= - 2 \sum_{n=1}^{N-1} H_n  \left( \frac{\pi n}{N} \right) \sin \left[ \pi n(k+1/2)/N\right] \\
&= - 2 \sum_{n=0}^{N-2} H_{n+1}  \left( \frac{\pi (n+1)}{N} \right) \sin \left[ \pi (n+1)(k+1/2)/N\right].
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Spectral Vanishing Viscosity}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The SVV operator as described in Karamanos is defined with a coefficient dependent on the grid size, essentially you get $\epsilon$ (proportional to $1/N$) multiplied by $Q$ (the filter). The filter cut off is something like $5\sqrt{N}$, or maybe I like $N^{3/4}$ just because it scales better.

I think that I don't want to build in the $\epsilon$ scaling into the definition of the operator because I damp, for example, with biharmonic damping. I think that's appropriate. The cutoff *should*, however, be performed automatically. I don't see any reason why not. So an SVV filter for a given set of dimensions is returned without any parameters needing to be specified. Of course, the 2/3 anti-aliasing should be built in automatically as well.

In a mixed basis we may have different values of wave numbers that are resolved, but we would still like the damping operator to be isotropic. Therefore, we need to define the filter cutoff in terms of an actual wavenumber. The sampleInterval is the Fourier frequency and the domainMin plus domainLength is the Nyquist frequency. Thus, we want to our filter cutoff to be a function of the Nyquist frequency.

if $k_{\textrm{max}}$ is the largest resolved wavenumber (taking into account anti-aliasing) the SVV operator prevents wave numbers below $k_{\textrm{cutoff}}=\Delta k \left(\frac{k_{\textrm{max}}}{\Delta k} \right)^{\frac{3}{4}}$ from being damped at all. Wavenumbers between the cutoff and the max have reduced damping by some factor $Q(k)$ where $0<Q(k)<1$. The wavenumber with $Q(k)$ reduction can be found with,
\begin{equation}
Q(k) = 
\begin{cases}
0 & k < k_{\textrm{cutoff}} \\
\exp \left[ - \left( \frac{k-k_{\textrm{max}}}{k-k_{\textrm{cutoff}}} \right)^2 \right] & k_{\textrm{cutoff}} < k < k_{\textrm{max}} \\
1 & k > k_{\textrm{max}}
\end{cases}
\end{equation}
where $k_m$ is the minimum Nyquist frequency of the various dimensions and $k_c = \Delta \left(\frac{k_m}{\Delta}\right)^{3/4}$. The filter will therefore gently activate wavenumbers above $k_c$ until $k_m$ at which they're fully activated.

\subsection{Example}

Consider two different variations of the cosine transform. In the first case, DCT-I, the cosine function is even around $n=0$ and $n=N-1$ where $n=0..N-1$. This means that if we define $x_n = n*\Delta$ where $\Delta = 1/(N-1)$, then $3\cos( 10 \cdot 2\pi)$ should give an amplitude of 3.

Alternatively, DCT-II, the cosine function is even around $n=-1/2$ and $n=N-1/2$. This means that if we define $x_n = (n+1/2)*\Delta$ where $\Delta = 1/N$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Linear Solvers
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Matrix Manipulation}

Major types of matrix manipulation include,
\begin{itemize}
\item linear solvers,
\item linear transforms,
\item eigenvectors,
\item matrix inversion,
\item tensor products.
\end{itemize}

\subsection{Linear Solvers}

We need to be able to solve the linear matrix equation,
\begin{equation}
M \vec{x} = \vec{b}
\end{equation}
for $\vec{x}$ given $M$ and $\vec{b}$. Component-wise, our transformations might look like,
\begin{equation}
M^{ijk}_{lmn} x^{lmn} = b^{ijk}
\end{equation}
where each dimension is identified with a component pair, $i-l$, $j-m$, $k-n$ which are defined be identity, dense, diagonal, etc.

Depending on whether the matrix is the identity, diagonal, tridiagonal, or dense, we need to choose a different solution algorithm. The matrix may be any one of these types in \emph{each} dimension, making the problem far more tricky. The current need is such that we only expect one dimension to be tridiagonal or dense, effectively meaning that we only need to solve the one-dimensional non-trivial problem. Specifically, we can solve the following types of equations,
\begin{enumerate}
\item All matrix dimensions are the identity.
\item All matrix dimension are the identity or diagonal.
\item One matrix dimension is tridiagonal, all others are the identity or diagonal.
\item One matrix dimension is dense, all others are the identity or diagonal.
\end{enumerate}

In the first case we simply return $\vec{b}$. In the second case, we simply have to divide $\vec{b}$ elements by the elements of $M$ (some repeated). In the third and fourth case, we need to loop over the elements in the diagonal dimensions, solving the tridiagonal or dense matrix problem in the other dimension.

For the solver we need to create a function which let's us focus on the one-dimensional problem, and let some outer-loop generalize that to the multidimensional case.

If a dimension of the linear transformation is
\begin{enumerate}
\item the identity, then the \emph{computation} needs to be repeated across that dimension because there will be new values of $b$.
\item a diagonal, then the \emph{computation} needs to loop across the diagonal.
\end{enumerate}

\subsection{Linear Transformations}

We need to be able to solve the linear matrix equation,
\begin{equation}
M \vec{x} = \vec{b}
\end{equation}
for $\vec{b}$ given $M$ and $\vec{x}$. Component-wise, our transformations might look like,
\begin{equation}
M^{ijk}_{lmn} x^{lmn} = b^{ijk}
\end{equation}
where each dimension is identified with a component pair, $i-l$, $j-m$, $k-n$ which are defined be identity, dense, diagonal, etc.

This follows nearly the same basic pattern as above, in that we need to isolate one dimension, and then the other dimensions we need to 
\begin{enumerate}
\item the identity, then the \emph{computation} needs to be repeated across that dimension because there will be new values of $b$.
\item a diagonal, then the \emph{computation} needs to loop across the diagonal.
\end{enumerate}

\subsection{Eigenvectors}

The eigenvector problem requires the same loops as the linear transformation problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Linear Transformations}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



Now it's time to implement finite differencing and more general linear transformations. These require generalizations of the concepts above.

A linear transformation is a tensor that transforms a vector from one basis to another. The Fourier transforms are simply linear transformations and we could, in fact, write down an $n \times n$ transformation matrix instead of using the FFT. A sufficient generalization then for now is to consider linear transformations that transformation from one set of dimension to another set the same length. In general this doesn't have to be true, but it's a good start.

Note there is still a distinction to be made between transformations to and from a kGLDeltaBasis and those transformations that go to and from a trigonometric basis. The reason is that the definition of differentiation is quite different, as the basis vectors need to be differentiated in the trigonometric basis.

Okay, so backing up a bit, FFTs and matrix operations are both types of linear transformations. In the former case we don't need to store an values in a variable, but only because our library that we're calling is doing this for us. In the latter case, we may need to store a complete matrix, or perhaps just a sparse subset of this.

Now, the differential operators are also interesting. They too are linear transformations that potentially involve changing the basis vectors, or not. So that part is not different. The only part that is different, is that follow Liebnitz rule. I think that's it. In the case of differentiating in a trigonometric basis, you simply multiply by a diagonal matrix. Differentiating in a delta basis may be multiplying by a tridiagonal matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Storing linear transformations
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Storing linear transformations}

Up until now, we have defined our variables as vectors with respect to some basis. This is, definitely, a concept separate from defining a linear transformation. On the other hand, a linear transformation is simply a bunch of these variables defined as a coefficient for each basis vector. In all cases, a linear transformation has a from-basis, and a to-basis. So I think that it's fair to define a linear transformation as a subclass of GLVariable that has slightly more structure.

Technically then, I suppose we might define a GLScalar, GLVector, GLMatrix---all of which are subclasses of GLTensor.

The puzzling thing about all of this is that my GLVariables, I have thought of as scalar functions defined on their dimensions. That *is* correct. There's nothing wrong with that thinking. And yeah, when I take derivatives, I'm taking derivatives of scalars. So my basis is a function basis. And that's why I shouldn't be thinking in terms of eigenvectors so much as eigenfunctions.

So let's go back. One linear transformation I'm interested in might transform a function from an existing basis to an eigenbasis. Another linear transformation might transform a function to its derivative. The concept of Linear transformation is fine here, and I see no reason to further mess with it.

Okay, now the issue becomes how to create an appropriate class for this. A linear transformation goes from one basis to another (maybe the same) basis. That must be part of the definition. It also has some storage needs for how to do this, maybe nothing, but maybe a lot. The question now is how to specify the storage needs---and just reuse most of what we've done already.

One key thing is that we've been defining our differential operators as straight variables. Of course, they should be diagonal linear transformations. This wouldn't change too much about how we do things. And it would formalize the equivalence between finite differencing and solving a matrix spectrally.

One could specify the columns, rows, or diagonals of this transform. So, specifying a diagonal, would be taking vector $v_i$ and turning it into the matrix transformation $v_l \delta^l_i$. The vector could easily be placed off-diagonal, $v_l \delta^l_{i-1}$.

Assuming that we're transforming from a dimension of length $n$ to one of length $n$, the storage requirements are such:

\begin{tabular}{r l l}
\hline
 Type & Form & Storage \\
\hline
% after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
  kGLIdentityMatrixFormat & $\delta^i_j$ & 0 \\
  kGLDenseMatrixFormat & $m^i_j$ & $n^2$  \\
  kGLDiagonalMatrixFormat & $b^i \delta^i_j$ & $n$ \\
  kGLSubdiagonalMatrixFormat & $a^i \delta^i_{j+1}$ & $n$ \\
  kGLSuperdiagonalMatrixFormat & $c^i \delta^i_{j-1}$ & $n$ \\
  kGLTridiagonalMatrixFormat & $a^i \delta^i_{j+1} + b^i \delta^i_j + c^i \delta^i_{j-1}$ & $3n$ \\
\hline
\end{tabular}

In the above table, the components $a_0$ and $c_{n-1}$ are \emph{always} zero.

Note that $v^j$ is a column vector where the index $j$ iterates through the rows. A transformation matrix is represented by $R^i_j$ where $i$ indexes the matrix row and $j$ indexes the matrix column.

For reference, the diagonal format takes the following form,
\begin{equation}
\left[\begin{array}{ccccc}b_0 & c_0 &  &  & 0 \\a_1 & b_1 & c_1 &  &  \\ & a_2 & b_2 & \ddots &  \\ &  & \ddots  & \ddots  & c_{n-2} \\0 &  &  & a_{n-1} & b_{n-1}\end{array}\right] \left[\begin{array}{c}x_0 \\x_1 \\x_2 \\ \vdots \\x_{n-1}\end{array}\right]
\end{equation}

\subsection{Matrix Multiplication}

The single diagonal formats make for very simple transformation operations. Take vector $v^i$ an considering how a sub-diagonal matrix acts on it:
\begin{equation}
w^i = a^i \delta^i_{j+1} v^j
\end{equation}
which means that,
\begin{align}
w^0 =& 0 \\
w^i = & a^i v^{i-1}, i \geqslant 1
\end{align}
So here the first multiplication is $w^1 = a^1 v^0$, meaning we shift the result and transform matrix.

A super diagonal matrix,
\begin{equation}
w^i = c^i \delta^i_{j-1} v^j
\end{equation}
which means that,
\begin{align}
w^i = & c^i v^{i+1}, i \geqslant 0\\
w^{N-2} =& 0
\end{align}
Here the first multiplication is $w^0 = c^0 v^1$, meaning we shift the operand.

\subsection{Tensor product}

We can define transformation or differentiation matrices on a single dimension, then scale up to multiple dimensions by taking the tensor product. This lets us define the derivative operators in a single dimension, then easily scale them up.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Multidimensional matrix solutions
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multidimensional matrix solutions}

Consider 


\section{Class Hierarchy}

It's proper to think of GLVariable objects as discrete functions. They are functions defined on some manifold with respect to some dimensions. We also have some GLVariables that are dimensionless that we've had to accommodate. Now we have Linear Transformations which are defined with respect to two sets of dimensions---those being transformed from, and those being transformed to. These linear transformations also happen to be operators that act on functions. This duality explains why it was a challenge to determine the best way to define differential operators (as a subclass of GLVariable, or a subclass of GLVariableOperation?).

\end{document}  